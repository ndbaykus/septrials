from torchvision import datasets, transforms
import torch
import torch.nn as nn
import numpy as np
from matplotlib import pyplot as plt
from torch.optim import Adam
from torch.utils.data import DataLoader
from tqdm import tqdm 
import time

# Settings
RANDOM_SEED = 1
LEARNING_RATE = 0.001
BATCH_SIZE = 128
NUM_EPOCHS = 10
NUM_CLASSES = 10
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Designing the Model
def conv(in_channels, out_channels, stride=1):
    return nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride)

class Block(nn.Module):
    def __init__(self, in_channels, out_channels, stride, downsample=None):
        super().__init__()
        self.conv1 = conv(in_channels, out_channels, stride)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.SiLU()
        self.conv2 = conv(out_channels, out_channels, stride=1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.downsample = downsample

    def forward(self, x):
        residual = x
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        if self.downsample is not None:
            residual = self.downsample(x)
        out += residual
        return self.relu(out)

class ResNet(nn.Module):
    def __init__(self, block, layers, labels):
        super().__init__()
        self.in_channels = 64
        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.SiLU()
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)

        self.layer1 = self.make_layer(block, 64, layers[0])
        self.layer2 = self.make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self.make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self.make_layer(block, 512, layers[3], stride=2)

        self.avgpool = nn.AdaptiveAvgPool2d((1,1))
        self.dropout = nn.Dropout(p=0.1)
        self.fc = nn.Linear(512, labels)

    def forward(self, x):
        out = self.maxpool(self.relu(self.bn1(self.conv1(x))))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = self.avgpool(out)
        out = torch.flatten(out, 1)
        out = self.dropout(out)
        return self.fc(out)

    def make_layer(self, block, out_channels, blocks, stride=1):
        downsample = None
        if stride != 1 or self.in_channels != out_channels:
            downsample = nn.Sequential(
                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
        layers = [block(self.in_channels, out_channels, stride, downsample)]
        self.in_channels = out_channels
        for _ in range(1, blocks):
            layers.append(block(self.in_channels, out_channels, stride=1))
        return nn.Sequential(*layers)

# Helper Funcs
def compute_accuracy(model, data_loader, device):
    model.eval()
    correct_pred, num_examples = 0, 0
    with torch.no_grad():
        for features, targets in data_loader:
            features, targets = features.to(device), targets.to(device)
            logits = model(features)
            predicted_labels = torch.argmax(logits, dim=1)
            correct_pred += (predicted_labels == targets).sum()
            num_examples += targets.size(0)
    return correct_pred.float() / num_examples * 100

# The Model
# In Main to compute parameters faster
def main():
    torch.manual_seed(RANDOM_SEED)

    # Transform and Dataset
    custom_transform = transforms.Compose([
        transforms.Resize((64, 64)), 
        transforms.ToTensor(),       
        transforms.Normalize((0.5,), (0.5,))
    ])

    train_dataset = datasets.MNIST(root='data', train=True, transform=custom_transform, download=True)
    test_dataset = datasets.MNIST(root='data', train=False, transform=custom_transform)

    # DataLoader (num_workers can be 2 if CPU is strong)
    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)

    # Model preparation
    neconet_model = ResNet(Block, [2,2,2,2], NUM_CLASSES)
    neconet_model.to(DEVICE)

    optimizer = Adam(neconet_model.parameters(), lr=LEARNING_RATE)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=1e-6)
    criterion = nn.CrossEntropyLoss()

    print(f"Training Starts... Device: {DEVICE}")
    
    for epoch in range(NUM_EPOCHS):
        neconet_model.train()
        running_loss = 0.0
        
        # Training loop with tqdm
        batch_loop = tqdm(train_loader, desc=f"Epoch {epoch+1}/{NUM_EPOCHS}")
        for targets, labels in batch_loop:
            targets, labels = targets.to(DEVICE), labels.to(DEVICE)

            # Forward
            logits = neconet_model(targets)
            loss = criterion(logits, labels)

            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            batch_loop.set_postfix(loss=loss.item())

        scheduler.step()
        
        # Success after each Epoch
        train_acc = compute_accuracy(neconet_model, train_loader, DEVICE)
        print(f"Epoch {epoch+1} Over | Loss: {running_loss/len(train_loader):.4f} | Train Acc: {train_acc:.2f}%")

    # Final Test
    test_acc = compute_accuracy(neconet_model, test_loader, DEVICE)
    print(f"\nRESULT - Test Accuracy: {test_acc:.2f}%")

if __name__ == '__main__':
    main()